\documentclass{amia}
\usepackage{lipsum} %Remove if not needed

\setlength{\bibsep}{0pt} %Comment out if you don't want to condense the bibliography

\begin{document}


\title{Using Restricted Systems in the Context of Collaborative Health Analytics Partnerships}

\author{Jacob S. Zelko, BS$^1$, Ashlin Harris, PhD$^2$, Shawn Imtiazuddin, MS$^3$, Matt Sanders, MS$^3$, Richard Starr, BS$3$, Megan Denham, MS$1,3$}

\institutes{
    $^1$Georgia Tech Research Institute, Atlanta, GA; $^2$Brown University, Providence, RI; $3$Georgia Institute of Technology, Atlanta, GA
}

\maketitle

\section*{Abstract}

\textit{Some profound abstract goes here}

\section*{Background}

\subsection{Overview of Restricted Environments}

As data breaches continue to be a threat to the integrity of a variety of analyses and collaborations, the need for secured environments continue to be an urgent requirement. Furthermore, with cyber attackes growing in recent years, securing an environment that houses sensitive information (such as patient information) is vital. Several approaches in recent years have been developed (such as fully airgapped systems, etc.) to meet these concerns.

Air-gapped systems represent the isolation of a set of resources, including physical networking and systems hardware, networks, applications, and data via the segmentation of all those components from the outside purview. The purpose of air-gapped systems is two-fold. Firstly, such systems are required by IRB and other requirements that are set forth by the data owners and governing bodies. Secondly, the proper protection of identifiable healthcare data cannot be realized in a non-regulated and public domain.

One other justification for air-gapped systems is protecting medical equipment from malware. While it is safe to say that air-gapped networks are less susceptible to low-effort attacks, the greatest threats are targeted breaches.

\subsection{Purpose of Restricted Environments}

\begin{enumerate}
	\item Common controls for restricted environments
	\item Requirements Localized for Deployments
\end{enumerate}

\subsection{Restrictions and Limitations}

Despite the robust security measures put in place within airgapped systems, the weakest link in the security chain is often the individual. The intentional or unintentional actions of an individual, such as opening a malicious email or sharing login credentials, can compromise the security of the entire system. To prevent such incidents, system administrators must regularly educate users on safe computing practices and provide guidelines for handling sensitive data. Additionally, strict access controls should be enforced to ensure that users can only access the data and systems they need for their work, and audit logs should be reviewed regularly to detect any unusual activity.

Another of the major challenges of working within airgapped systems is the greater level of scrutiny placed on system administrators and data owners to prevent data leakages from the protected environment. The use of airgapped systems is driven by the need to protect sensitive healthcare data, so any breach or leak of this data could have serious consequences for both the individuals affected and the organizations responsible for protecting the data. System administrators and data owners must ensure that all necessary controls are in place to prevent unauthorized access to the system and data. This includes implementing strict access controls, monitoring all activity within the system, and conducting regular security audits.

Another challenge is the strict rules and regulations that must be adhered to based on the type of data that is housed in the environment. Different types of data may be subject to different controls, and there may be multiple standards and certification programs that must be followed. For example, healthcare data may be subject to controls set forth by governing bodies such as NIST, as well as additional controls mandated by standards like HITRUST. These controls may include encryption, access controls, and other measures to protect the confidentiality, integrity, and availability of the data.

In addition, the network and systems within airgapped environments are designed to operate under the deny all outbound traffic by default rule. This means that system administrators may not freely download operating system or third-party application updates from public-facing vendor-supported websites. Instead, local repositories must be utilized where it makes sense for airgapped systems to pull updates from those repositories. Any changes to allow communication to the outside, whether temporary or permanent, requires the intervention and audit from the compliance team. Users also face a similar challenge as they are not able to easily pull code from places like GitHub and other public repositories. Instead, a more manual download then import process is generally enforced. Additionally, there are several environmental controls that result in decreasing a user's "ease of use" factor, such as multiple touchpoints for users in order to access the environment, and troubleshooting for both users and administrators can be challenging.

Overall, these challenges underscore the importance of careful planning and implementation of airgapped systems for healthcare research. While airgapped systems provide important protections for sensitive healthcare data, they require a significant investment of time, effort, and resources to implement and maintain.

When it comes to preparing air-gapped solutions in healthcare, it's crucial to keep in mind that the efficiencies that may exist in a regular working environment may not exist in the Air-Gapped environment. The additional layers of security and restrictions can make even simple tasks more complex and time-consuming. This means that application owners must consider how the application may function differently in an Air-Gapped environment.

For example, in an air-gapped environment, centralized or shared repositories for applications such as R, Python, Julia, etc. are necessary. This allows for the application owners to control which versions of a particular package are allowed to be used within the environment. Any request for a new package for an application must be audited then imported by the compliance and operations teams. This ensures that only approved and secure packages are used in the system, and helps to prevent the introduction of vulnerabilities or security risks.

Updates to a Virtual Desktop-based infrastructure also add additional levels of complexity to the import/update process. The updates must be carefully vetted and tested to ensure they do not introduce new security risks or vulnerabilities. Once approved, the updates must be imported into the system in a way that minimizes disruption to ongoing work and avoids the introduction of errors or compatibility issues. All of these steps require careful coordination and planning by the application owners and compliance teams, as well as a high degree of expertise and technical knowledge.

Ultimately, the importance of preparing air-gapped solutions in healthcare cannot be overstated. The sensitive and confidential nature of healthcare data means that protecting it from unauthorized access or theft is of paramount importance. This requires a combination of robust security protocols, careful planning and coordination, and ongoing monitoring and evaluation to ensure that the system remains secure and effective over time. By taking a proactive and vigilant approach to air-gapped solutions, healthcare organizations can ensure the integrity and safety of their data, while also meeting regulatory and compliance requirements.

\section{Methodologies}

\subsection{Exploratory Data Analysis and Development}

Within exploratory data analysis (EDA) workflows, it is immensely common for researchers or data analysts to utilize languages like Python to build such analyses.
What is common practice in the context of EDA workflows is that often, languages like Python do not include data science packages within their standard library.
Instead, analysts rely on external third part libraries to perform these analysis tasks.
Furthermore, Python has an extensive data science ecosystem and a plethora of package managers to quickly download and begin using these tools in EDA workflows.

However, a drawback to this approach is the danger of security in the software someone may install from outside of the language standard libraries.
For Python, packages are provisioned through PyPi which has some modest security measures to safeguard against malicious code being uploaded to the PyPi software download registry.
Even still, one popular form of attack is to "typo squat" the names of popular data science packages, such as "pandas", to trick someone into installing a malicious package.
If someone installed "padnas" by mistake, the installation most likely will succeed but then the malicious software will then be made available on the analyst's machine where it could then cause significant potential damage.

Another aspect within the EDA workflow process is the notion of reproducibility of EDA pipelines.
A language such as Julia is able to record quickly what packages, package versions, and dependencies an EDA pipeline has through the use of various configuration files (a Project and Mainfest toml in the case of Julia). 
Then, as long as these configurations are up to date, one can copy this workflow to another machine and reproduce it exactly. 

Important to note is that, like the installation of third party packages, there are limitations in how pipelines can be reproduced. 
The architecture between machines largely should be the same or, at least, and packages in the EDA workflow should be flexible to handle different operating system or hardware platforms.
If not, the EDA workflow may fail entirely or lead to unexpected results or errors. 
Additionally, for languages that are used in EDA pipelines, generally the same version of the language that was used to develop the pipeline must also be used in rebuilding the pipeline on a host machine.
If the versions do not match, it is possible that various errors will occur and ruin the ability for reproducibility for the EDA  pipeline -- both for using it as well as potentially auditing it in the future. 

Another important consideration to take into account when working with EDApipelines is to also think about perforamnce requirements when working with large amounts of data fed forawrd through such a pipeline.
In the case of R, the programming language ecosystem there has a strong suppport and bindings for more performant languages such as C++. 
Using such interfaces, R can become immensely fast and with the addition of tools from the tidyverse, also ergonomic in the utilization of these interfaces in a simple manner.

This sort of concern does definitely depend on the context of where an EDA pipeline takes place. 
There can be a number of limitations that might make such implementations of an EDA  pipeline uneffective on a new machine. 
This includes how the architecture of the build system is constructed, what sort of hardware is available, and if resources on the machine where the EDA  pipeline is deployed to has to be shared between many different pipelines or users. 

Finally, arguably the most important aspect of developing an EDA pipeline for deployments is the ability to back up and version the developments made for the pipeline.
The de facto tool of choice is the tool called, "git", and it is used to back--up snapshots and changes of a given code project at points throughout the existence of the EDA development process.
Git allows you to create a log of changes that propoagates through the history of theproject giving one easy access to the design decisions that were made to create the package as it currently exists and allows one to quickly revert changes if any new feature added caused an error with the EDA  pipeline.
Additionally, this gives teams the ability to work on the pipeline in a distributed manner with many persons working on the same or different aspects of the pipeline.

Yet with this approach, there are not so much issues with the concept of version control itself in practice but the set-up it requires.
Version control generally requires one to back up code to a hosting platform such as GitHub or GitLab.
This is fine, but requires an internet connection in practice.

\subsection{Collaboration and Network Studies (Jacob)}

\subsection{Contrast Limitations from Restricted Environments (Shawn)}
        \begin{enumerate}

There are several scenarios where typical workflow operations for a researcher may not carry over into multi-user working environments.  Regardless of the types of controls that may be enforced in a given working environment, what is clear is that the workflow will differ for a user if that user is working in a shared space as opposed to their own personal device.  Technologies offered from vendors such as Microsoft, VMware, Citrix, and others that leverage scalability in a multi-user environment have shown us that every piece of software solution that is being considered and offered in the environment must be compatible in certain respects to have fully functional and operable work environments.  Those requirements are often more enhanced in restricted environments that do not allow hosts outbound by default.

So what does this mean?  Well, lets take Python, for example.  System administrators working in multi-user environments must consider multiple factors when installing Python onto gold images.  First, can the software be installed for all users as opposed to a single user?  This is crucial as installing software for all users generally means the ability of the local software package repository to be shared across multiple users.  This means the ability for a system administrator to install a package on the local repository which then becomes available to use for all users without running into permissions issues.  Second, can you update these packages in an efficient manner that does not suffer from time consuming and tedious tasks?  If a system administrator is spending hours simply updating one package at a time every month, then this clearly becomes a compliance and operational headache.  Luckily, we don't have to worry about that with Python.  Third, and this relates to an additional layer of security that restricted and controlled environments will contend with, and that is the ability for software packages to be vetted before download.  Vendors such as JFrog specialize in this area as their Artifactory / XRay platforms allow for the proper vetting of packages for vulnerabilities before they are downloaded.  It also means that a researcher and/or a system admin is not downloading the package from a public and possibly unprotected repositories as all approved packages would be pulled from the platforms mentioned above.  In restricted environments, the alternative might be that the package would need to be manually downloaded, vetted, then installed into the environment, which again, can be a time consuming and tedious process. 

        \end{enumerate}


\section*{Recommendations and Best Practices}

\subsection{System Administration to Enforce Proper Operations and Compliance (Shawn)}

        \begin{enumerate}
            \item ...
        \end{enumerate}


\subsection{Researcher compliance (Jacob/Shawn)}
        
\subsubsection{Restricted Environment Exploratory Data Analysis}

\subsubsection{Restricted Environment Software Development }

\section*{Discussion}

A. Biosurveillance and Outbreak Management 

C. Evaluating Public Health Programs 

D. Modeling the Spread of Disease 

E. Understanding the Epidemiology of Disease 

\subsection*{Site Collaboration}

Shawn and Jacob speak on lessons learned in collaborating in air-gapped environments. 

Restricted environments can, at times, be frustrating to work with in collaborative efforts. This can be for a number of reasons such as ....

Maintenance cylces: routine maintenance that is part of maintaining a restricted environment can be at odds with collaborative efforts. For example, if a secure environment is going to be down for a few days in a work week, this can delay urgent analysis efforts needed in a collaboration. Additionally, interruptions caused by maintenance can completely pause or stop an analysis. 

To work around this, there are various methods for collaborators can pursue to mitigate disruptions. These stages could then be completed by a site collaborator who has a restricted environment in between maintenance windows. 

Automating export and suppression procedures:

Provide documentation about site specifics to mitigate debugging by site collaborator:



\subsection{Balancing Privacy with Usability}

\section*{Conclusion}

Your conclusion goes at the end, followed by References.  References begin below with a header that is centered.  Only the first word of an article title is capitalized in the References.

\section*{Acknowledgments}

% References as numbers
\makeatletter
\renewcommand{\@biblabel}[1]{\hfill #1.}
\makeatother

% unstr is used to keep citation order
\bibliographystyle{vancouver}
\bibliography{amia}  

\section*{Appendix}

\subsection{Georgia Tech Protected Health Data Infrastructure}

\subsubsection{Infrastructure Description}
\subsubsection*{PHDI Architecture}

The first step in setting up an airgapped system is the profile set-up and management process. This involves configuring the system's security settings, user accounts, and access controls.  

The staging environment is used to prepare and test software before deployment. An important distinction about the PHDI Staging environment is that it does not connect to any database or import any protected data. However, it does have internet access for pulling required content for analyses. The staging environment is ephemeral and can be destroyed, allowing for easy rollbacks and recovery in case of issues. 

Deployment involves using a gold image, which is a pre-configured system image that contains all the necessary software and configurations for the airgapped system. The gold image is also somewhat ephemeral, meaning it can be easily replaced or updated without affecting the rest of the ABC system. When the gold image is deployed, any content that was created within the staging environment can be included within the gold image. 

\subsubsection{Considerations}

\subsection*{Practical Considerations for Popular Data Science Languages}

\subsubsection*{Python}

\subsubsection*{R}

\subsubsection*{Software Management}

Since airgapped systems are isolated from external networks, software must be managed internally and shared across environments. This includes configuring and testing software in the staging environment before deploying it via a gold image. 

\paragraph{R Package Environments}

When working with projects that rely on the R programming language, a variety of issues can propogate:

\begin{enumerate}
	\item Reproducible project environments: Reproducible project management tools (such as renv or packrat) will fail to reproduce the project environment once deployed into an airgapped system. For example, these are the steps to create and instantiate an R based project using the tool, `renv`:
		\begin{enumerate}
			\item Call `renv::init()` to initialize a new project environment.

			\item Install and remove R packages into the project as needed.

			\item Execute `renv::snapshot()` to save the state of the project to `renv.lock`.

			\item Use `renv::restore()` to revert to the previous state encoded in the lockfile if needed or to instantiate (i.e. install required project packages) an already existing project for the first time.
		\end{enumerate}
	Although this system works great for use when it is guaranteed one has network connection, this will fail for airgapped systems as all `renv` will do is consistently record R version packages. Some complex workarounds do exist for this problem but often break or are not easily reproduced. 
	\item Package Requirements: Packages required for projects may not be previously approved or audited for import into an airgapped environment. This can have a deletrious effect on the efficacy of not only running a project but also effective and timely collaboration. 
	\item Non-Standard Architecture: Due to the workarounds needed at times for working with R projects inside an airgapped system, standard architecture that R relies on can become unreliable. One crucial example is for multi-threaded R processes. These processes may be unable to resolve proper project paths and package locations due to non-standard locations.
\end{enumerate}

Some suggested solutions for handling these problems are listed below:

\begin{enumerate}
	\item An approach that was found to be effective for actually ensuring the reproducibility of an R project for an airgapped environment was to use the following approach:
	\begin{enumerate}
		\item Move, clone, or upload the R project into a development environment (it is assumed that this development environment has internet connection and system architecture that matches exactly to the production environment).
		\item Navigate into the project and use `renv` to instantiate and download the project package dependencies.
		\item Locate where exactly these packages were installed onto the development machine and zip them into a zip file.
		\item Move the R project into the production environment and disable `renv` (this is to make `renv` stop trying to force a connection to the internet or throw connection errors).
		\item Move the package zip file into the production environment and unzip it into a single directory.
		\item Source the package directory from the R package as an external or different library. 
		\item Proceed with analysis or experiments now with this configuration.
	\end{enumerate}
	This will enable one to work with the required packages and package versions for a given R project. 
	\item A potential approach that can address audit requirements is to set-up a custom site-specific CRAN registry. This registry can be established to enforce auditing procedures and to ensure that any package brought into an environment is benign. Additionally, such registries could actually be connected to a protected environment enabling users to install required packages as needed in much the same way of installing packages normally outside a protected environment. 
	\item To address the often nonstandard architecture of R projects in a protected environment, one can set a package .Rprofile with appropriate paths. This .Rprofile can then propagate across processes ensuring that all spawned processes have access to required paths and packages for tasks to be performed on that process.
\end{enumerate}

\paragraph{Julia Package Environments}

When working with projects that rely on the Julia programming language, a variety of issues can propogate:

\begin{enumerate}
	\item Reproducible project environments: One of the strongest assets to the Julia ecosystem is its ability to manage and version projects that are written in Julia. Tools (such as the native Pkg library and DrWatson.jl) for reproducible projects in Julia depend on two files being the Project.toml, which specify what packages and Julia version the user of the project will need, and a Manifest.toml, which specifies every single dependency required by every single package within a project. However, similar to R, 
\end{enumerate}

Julia packages can be used in airgapped systems, but virtual environments can be thwarted by shared environments. In such cases, system images could be built within the staging environment. This allows developers to test software configurations thoroughly and ensures that they are secure before deployment. 

\subparagraph*{Julia}

Julia's package manager relies on internet access, so it can be challenging to make dependencies available on secure systems. One solution is to use containers, but the issue can also be solved within Julia. PackageCompiler.jl compiles Julia code ahead of time, primarily for improved performance in certain workflows. In particular, generating a sysimage produces a performant stand-alone Julia session, and since its main drawback (version locking) is mitigated on secure systems, it seems to be the best approach for putting a Julia session on a system without internet or ssh access available to users. 

\end{document}
